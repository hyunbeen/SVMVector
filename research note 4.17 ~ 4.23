4월 3째주 연구노트 - Multiclass SVM
Binary (two-class) classification using support vector machines (SVMs) is a very well developed technique.
Due to various complexities, a direct solution of multiclass problems using a single SVM formulation is usually avoided. 
The better approach is to use a combination of several binary SVM classi- fiers to solve a given multiclass problem.
Popular methods for doing this are: one-versus-all method using winner-takes-all strategy (WTA SVM); 
one-versusone method implemented by max-wins voting (MWV SVM); DAGSVM; and error-correcting codes. 
Hastie and Tibshirani proposed a good general strategy called pairwise coupling for combining posterior
probabilities provided by individual binary classifiers in order to do multiclass classification.
Since SVMs do not naturally give out posterior probabilities, they suggested a particular way of generating these probabilities
from the binary SVM outputs and then used these probabilities together with pairwise coupling to do muticlass classification. 
Hastie and Tibshirani did a quick empirical evaluation of this method against MWV SVM and found that the two methods give 
comparable generalization performances. Platt  criticized Hastie and Tibshirani’s method of generating posterior class probabilities
for a binary SVM, and suggested the use of a properly designed sigmoid applied to the SVM output to form these probabilities. 

An Empirical Study 279 use of Platt’s probabilities in combination with Hastie and Tibshirani’s idea of pairwise coupling has not 
been carefully investigated thus far in the literature. The main aim of this paper is to fill this gap. We did an empirical study 
and were surprised to find that this method (we call it as PWC PSVM) shows a clearly superior generalization performance over MWV SVM 
and WTA SVM; the superiority is particularly striking when the training dataset is sparse. 

M will denote the number of classes and ωi, i = 1,...,M will denote the M classes. For binary classification we will refer to the
two classes as positive and negative; a binary classifier will be assumed to produce an output function that gives relatively large values
for examples from the positive class and relatively small values for examples belonging to the negative class. 2.1 WTA SVM WTA SVM 
constructs M binary classifiers. The ith classifier output function ρi is trained taking the examples from ωi as positive and the examples
from all other classes as negative. For a new example x, WTA SVM strategy assigns it to the class with the largest value of ρi.
2.2 MWV SVM This method constructs one binary classifier for every pair of distinct classes and so, all together M(M − 1)/2 binary
classifiers are constructed. The binary classifier Cij is trained taking the examples from ωi as positive and the examples from ωj 
as negative. For a new example x, if classifier Cij says x is in class ωi, then the vote for class ωi is added by one. Otherwise,
the vote for class ωj is increased by one. After each of the M(M −1)/2 binary classifiers makes its vote, MWV strategy assigns x
to the class with the largest number of votes.
