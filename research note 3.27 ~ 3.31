3월 5째주 연구노트 - SVM 특성

 SVM은 분류(classification)이나 회귀 분석(regression)에 사용이 가능하며, 특히 분   류 쪽의 성능이 뛰어나기 때문에 주로 분류에 많이 사용이 된다.
 SVM은 기본적으   로 지도 학습 알고리즘이며, hyper-plane(초평면)을 이용해 카테고리를 나뉜다. 또   한 SVM은 선형적 분류(Linear separability)의 
 유뮤에 따라 두가지로 나뉜다.

 Support Vector 와 Hyper-plane에 대해 설명해보자면 아래 그림5의 예처럼, 빨간 삼각형과 파란 점으로 구성된 집단을 나눈다면 어느 것이 가장 최적으로
 분류한 것일까? 이 부분 역시 대부분 직관적으로 오른쪽 아래에 있는 분류가 가장 잘 된 것이라고 판단할 것이다. 
 최적으로 나누는 것은 어떻게 할 수 있을까?

그 해답은 SVM에 답이있다. SVM은 분류를 할 때 최고의 마진을 가져 가는 방향으로 분류를 수행한다. 마진이 크면 클수록 학습에 사용하지 않은 
새로운 데이터가 들어오더라도 잘 분류할 가능성이 커지기 때문이다. 이것은 SVM 뿐만아니라 모든 머신 러닝 학습 방법이 추구하는 방향이다.


2차원이기 때는 직선이지만, 3차원 이상이 되면 평면이 되어 이것을 초평면(hyper-plane)이라고 부른다.
hyper-plane으로부터 가장 가까이 있는 파란색 점과 빨간색점을 Support vector이라고 부른다. 마진의 폭은 2/||w|| 이며, 마진이 최대가 되려면 
결과적으로 ||w||가 최소가 되도록 최적화 해줘야한다. ||w||가 최소가 된다는 뜻은 결과적으로는 ||w||의 제곱이 최소가 되는 것이기 때문에
quadratic optimization 과정으로 볼 수 있게 된다. quadratic optimization은 대부분의 머신 러닝 알고리즘들이 최적화 방법으로 사용을 하고 있다. 
