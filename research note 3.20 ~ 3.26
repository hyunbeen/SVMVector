3월 4째주 연구노트 - SVM이란

SVM은  “Support Vector Machine”의 약어로 이름만 보면 어떤 알고리즘 인지 느낌이 확 오지 않을 수도 있겠지만, Support Vector 와 hyper-plane dl
주요 개념인 머신 러닝 알고리즘의 하나이며, 원 알고리즘은 오래 전에 개발이 되었지만, 90년대 들어 새롭게 각광을 받으면서 많이 쓰이고 있다. 

신경망에 비해 간결하면서 뛰어난 성능을 보였기 때문에, 한동안 신경망의 암흑기를 가져다 준 대표 머신 러닝 알고리즘 중 하나이다. 
지지벡터기계가 학습한 분리 평면은 서로 다른 클래스에서 결정되는 소수의 지지벡터에 따 라 결정되기 때문으로 분석된다. 그러나 클래스 불균형 문제 에서
지지벡터기계가 학습한 분리 함수는 클래스의 사전 분포 를 반영하지 못하므로 긍정 클래스에 근접한 분리 함수가 학습되어, 테스트에서 긍정 클래스의 예측율이
부정 클래스의 데이터의 예측율보다 낮게 평가되는 경향이 있었다. 지지벡터기계 알고리즘의 개선은 목적함수의 변형, 학습시 커널함수의 수정, 샘플링 기법을
이용 한 긍정 클래스의 데이터를 추가 또는 삭제하는 학습 전략 등이 수행되었다. 목적함수의 변형에서는 이차 최적화 문제의 긍정 클래스의 정규화 변수 값을
부정 클래스 의 정규화 변수 값과 다르게 부여하여 부정 클래스에 근접하 는 평면을 학습시킬 수 있다. KBA 알고리즘은 지지벡터기계가 학습한 분리 평면을 
커널행렬을 수정하여 부정 데이터에 가깝게 이동시킨다.

GSVM(Granular-SVM), SMOTE+SVM 등 학 습 전략에서는 학습 데이터의 준비 단계에서 과도샘플링 (oversampling) 또는 과소샘플링(undersampling) 기법을
도입하여 두 클래스의 데이터 수를 동등하게 만든 후 분리 평 면을 학습한다. SMOTE+SVM는 이차 최적화 문제의 학습 시 긍정과 부정 클래스의 정규화 
값을 다르게 부여하는 학습도 병행하였다 . 그러나 과도샘플링 기법의 도입은 새로운 데이터 가 증가하며서 지지벡터의 수가 증가하는 경향이 분석되었다.
불균형 문제를 위한 지지벡터기계의 성능 향샹을 위한 연구 를 요약하면 클래스 불균형이 나타나는 분류 문제의 일반화 성 능을 높이기 위해 샘플링 기법의
도입은 적용하기 쉬운나 부정 클래스의 과소샘플링은 정보 손실로 인해 전체적인 성능에 영향을 줄수 있다. 긍정 클래스의 데이터를 추가하는 과도샘플링 
기법의 도입은 효과가 실험적으로 입증되었으나 새로운 데이 터 생성과 학습 시간의 증가 등 부가적 비용이 요구된다. 본 논문의 목적은 불균형 분류 
문제의 이차 최적화 문제에 대한 기 수행된 연구 결과를 분석하고 SMO 알고리즘의 개선 방안을 제안하는데 있다. 개선된 SMO는 최적해의 대상인 
두 라그랑쥐 승수의 선택 전략과 근접해의 상한과 하한을 결정하는 방법을 이용한다.
